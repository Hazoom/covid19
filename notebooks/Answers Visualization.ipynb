{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from resources.similarity import CovidSimilarityResource\n",
    "from corpus_index import load_corpus_index\n",
    "from encoders.simple_encoder import SimpleEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_index_folder = '../../../workspace/kaggle/covid19/data/corpus_index_no_remove_principles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../answers/1.json', 'r') as in_fp:\n",
    "    seed_sentences_json = json.load(in_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is known about transmission, incubation, and environmental stability?\n"
     ]
    }
   ],
   "source": [
    "print(seed_sentences_json['taskName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_index_fname = os.path.join(corpus_index_folder, 'simple-encoder-nmslib-100d.bin')\n",
    "corpus_index = load_corpus_index(corpus_index_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../workspace/kaggle/covid19/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.read_csv(os.path.join(data_dir, \"sentences_with_metadata_no_phrases_blingfire.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_articles_metadata_mapping(sentences_df: pd.DataFrame) -> dict:\n",
    "    sentence_id_to_metadata = {}\n",
    "    for row_count, row in sentences_df.iterrows():\n",
    "        sentence_id_to_metadata[row_count] = dict(\n",
    "            paper_id=row['paper_id'],\n",
    "            cord_uid=row['cord_uid'],\n",
    "            source=row['source'],\n",
    "            url=row['url'],\n",
    "            publish_time=row['publish_time'],\n",
    "            authors=row['authors'],\n",
    "            section=row['section'],\n",
    "            sentence=row['sentence'],\n",
    "        )\n",
    "    return sentence_id_to_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id_to_metadata = create_articles_metadata_mapping(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = SimpleEncoder.load(\n",
    "    \"../../../workspace/kaggle/covid19/data/fasttext_no_subwords_no_phrases_blingfire/word-vectors-100d.txt\",\n",
    "    \"../../../workspace/kaggle/covid19/data/fasttext_no_subwords_no_phrases_blingfire/word-counts.txt\"\n",
    "    # os.path.join(corpus_index_folder, \"simple-encoder-100d-components.npy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import text_tokenizer\n",
    "from nlp.cleaning import clean_tokenized_sentence\n",
    "\n",
    "def clean_sentence(sentence) -> str:\n",
    "    doc = text_tokenizer.tokenize_text(sentence)\n",
    "    tokens = [str(token) for token in doc]\n",
    "    return clean_tokenized_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import murmurhash\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "\n",
    "from corpus_index.base_index import Aggregation\n",
    "from nlp.textrank import calc_textrank\n",
    "\n",
    "class CovidSimilarity:\n",
    "    def __init__(self, corpus_index, sentence_encoder, sentence_id_to_metadata, bigram_model=None, trigram_model=None):\n",
    "        self.corpus_index = corpus_index\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.sentence_id_to_metadata = sentence_id_to_metadata\n",
    "        self.bigram_model = bigram_model\n",
    "        self.trigram_model = trigram_model\n",
    "\n",
    "    def similar_k(self, input_sentences, limit=10, method='union', group_by='cosine'):\n",
    "        \"\"\"Find similar sentences.\n",
    "\n",
    "        Args:\n",
    "            input_sentences (str/list[str]): one or more input sentences.\n",
    "            sentence_encoder  : encoder\n",
    "            limit (int): limit result set size to ``limit``.\n",
    "            corpus_index : type of corpus where to fetch the suggestions from\n",
    "            db_session  : Database to get neighbors from\n",
    "            method (str): aggregation method ('union', 'mean', 'pc1', 'pc2').\n",
    "            group_by (str): distance metric to use to group the result set. Default is 'cosine'.\n",
    "\n",
    "        Returns:\n",
    "            list<dict>\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        nearest = dict()\n",
    "\n",
    "        if method == 'textrank':\n",
    "            _, _, _, phrase_list = calc_textrank(input_sentences, num_phrases=10)\n",
    "            input_sentences = [' '.join(phrase[0] for phrase in phrase_list)]\n",
    "            method = Aggregation.UNION\n",
    "\n",
    "        cleaned_sentences = [clean_sentence(sentence) for sentence in input_sentences]\n",
    "\n",
    "        if self.bigram_model and self.trigram_model:\n",
    "            tokenzied_sentences = [sentence.split(' ') for sentence in cleaned_sentences]\n",
    "            sentences_with_bigrams = self.bigram_model[tokenzied_sentences]\n",
    "            sentences_with_trigrams = self.trigram_model[sentences_with_bigrams]\n",
    "            cleaned_sentences = [' '.join(sentence) for sentence in sentences_with_trigrams]\n",
    "\n",
    "        embeddings = self.sentence_encoder.encode(cleaned_sentences)\n",
    "\n",
    "        for idx, dist in self.corpus_index.knn_query_batch(embeddings, limit=limit, method=method):\n",
    "            similarity = 1.0 - dist\n",
    "            if idx not in nearest:\n",
    "                nearest[idx] = similarity\n",
    "            else:\n",
    "                nearest[idx] = max(nearest[idx], similarity)\n",
    "\n",
    "        print(nearest)\n",
    "        \n",
    "        results = []\n",
    "        for idx, sim in nearest.items():\n",
    "            if sim >= 0.5:\n",
    "                results.append(dict(similarity=sim, metadata=self.sentence_id_to_metadata[idx]))\n",
    "\n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"sentencs\": cleaned_sentences\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'results': sorted(res, key=lambda x: x['dist']),\n",
    "            'sentences': [\n",
    "                {\n",
    "                    'id': sent_id,\n",
    "                    'text': sent\n",
    "                } for sent_id, sent in zip(indices, cleaned_sentences)\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_similarity = CovidSimilarity(corpus_index, sentence_encoder, sentence_id_to_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(sentences, method=\"textrank\", limit=10):\n",
    "    return covid_similarity.similar_k( sentences, method=method, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{82747: 0.8962383270263672, 217386: 0.6820434331893921, 217362: 0.6366391777992249, 217378: 0.6274011135101318, 26668: 0.0, 53909: 0.0, 57795: 0.0, 58960: 0.0, 59899: 0.0, 63965: 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': [{'similarity': 0.8962383270263672,\n",
       "   'metadata': {'paper_id': 'c097a8a9a543d69c34f10e5c3fd78019e560026a',\n",
       "    'cord_uid': 'mn0l7nar',\n",
       "    'source': 'PMC',\n",
       "    'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7067204/',\n",
       "    'publish_time': '2020-01-28',\n",
       "    'authors': 'Jasper Fuk-Woo Chan, Kin-Hang Kok, Zheng Zhu, Hin Chu, Kelvin , Kai-Wang To, Shuofeng Yuan, Kwok-Yung Yuen',\n",
       "    'section': 'abstract',\n",
       "    'sentence': 'Learning from the roles of civet in SARS and camel in MERS, hunting for the animal source of 2019-nCoV and its more ancestral virus would be important for understanding the origin and evolution of this novel lineage B betacoronavirus.'}},\n",
       "  {'similarity': 0.6820434331893921,\n",
       "   'metadata': {'paper_id': 'ffbd51670f3a5dcf4a02696788726a3531da449b',\n",
       "    'cord_uid': 'si7csqr2',\n",
       "    'source': 'Elsevier',\n",
       "    'url': 'https://doi.org/10.1016/j.jmii.2020.03.011',\n",
       "    'publish_time': '2020-03-14',\n",
       "    'authors': 'Muh-Yong Yen, Jonathan Schwartz, Shey-Ying Chen, Chwan-Chuen King, Guang-Yang Yang, Po-Ren Hsueh',\n",
       "    'section': 'body',\n",
       "    'sentence': \"Recently, Taiwan's COVID-19 response was praised in public health circles. 12 Taiwan's success can be at least partially attributed to its success in breaking the communityhospital-community transmission cycle by implementing TCB with the enhancements identified here.\"}},\n",
       "  {'similarity': 0.6366391777992249,\n",
       "   'metadata': {'paper_id': 'ffbd51670f3a5dcf4a02696788726a3531da449b',\n",
       "    'cord_uid': 'si7csqr2',\n",
       "    'source': 'Elsevier',\n",
       "    'url': 'https://doi.org/10.1016/j.jmii.2020.03.011',\n",
       "    'publish_time': '2020-03-14',\n",
       "    'authors': 'Muh-Yong Yen, Jonathan Schwartz, Shey-Ying Chen, Chwan-Chuen King, Guang-Yang Yang, Po-Ren Hsueh',\n",
       "    'section': 'body',\n",
       "    'sentence': 'When coupled with strict PPE use and standard infection control procedures, hospital fomite, contact and droplet transmissions were efficiently controlled.'}},\n",
       "  {'similarity': 0.6274011135101318,\n",
       "   'metadata': {'paper_id': 'ffbd51670f3a5dcf4a02696788726a3531da449b',\n",
       "    'cord_uid': 'si7csqr2',\n",
       "    'source': 'Elsevier',\n",
       "    'url': 'https://doi.org/10.1016/j.jmii.2020.03.011',\n",
       "    'publish_time': '2020-03-14',\n",
       "    'authors': 'Muh-Yong Yen, Jonathan Schwartz, Shey-Ying Chen, Chwan-Chuen King, Guang-Yang Yang, Po-Ren Hsueh',\n",
       "    'section': 'body',\n",
       "    'sentence': 'These patients are transferred to the quarantine ward directly from outdoor triage and are held there for the full incubation period. 6 (Fig. 2) .'}}],\n",
       " 'sentencs': ['host civet palm merscov think originate facilitate colony bat']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_sentences([\"Both SARS-CoV and MERS-CoV are thought to have originated in colonies of bats, eventually transmitted to humans, putatively facilitated by intermediate hosts such as palm civets and dromedary camels, respectively (Cui et al., 2019) .\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid19",
   "language": "python",
   "name": "covid19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
